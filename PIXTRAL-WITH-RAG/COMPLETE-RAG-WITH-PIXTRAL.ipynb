{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client.models import PointStruct\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "qdrant_client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"MISTRALAI_API_KEY\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(url):\n",
    "    model = \"pixtral-12b-2409\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Extract the text from the image precisely, extract every text\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": url\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Get the chat response\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    # return the context\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(context, model = \"mistral-embed\"):\n",
    "    embeddings_batch_response = client.embeddings.create(\n",
    "        model=model,\n",
    "        inputs= context\n",
    "    )\n",
    "    return embeddings_batch_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(context: str):\n",
    "    model = \"mistral-embed\"\n",
    "    context = context.split('\\n')\n",
    "\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    client = Mistral(api_key=api_key)\n",
    "    \n",
    "    embeddings_batch_response = embed(context, model)\n",
    "\n",
    "    for i in range(len(embeddings_batch_response.data)):\n",
    "        temp = []\n",
    "        \n",
    "        temp.append(context[i])\n",
    "        temp.append(embeddings_batch_response.data[i].embedding)\n",
    "        \n",
    "        data.append(temp)\n",
    "\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_qdrant(length: int):\n",
    "    vector_size = length\n",
    "    # Define the vectors configuration\n",
    "    vector_params = VectorParams(\n",
    "        size=vector_size,                # Size of the vectors\n",
    "        distance=Distance.COSINE         # Choose distance metric (COSINE, EUCLID, or IP)\n",
    "    )\n",
    "    \n",
    "    # Create the collection with the specified configuration\n",
    "    if \"CHATBOT\" not in qdrant_client.get_collections().collections[0].name:\n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=\"CHATBOT\",\n",
    "            vectors_config=vector_params  # Specify vector configuration\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qdrant_entry(final_data):\n",
    "    points=[PointStruct( id=i,  vector=final_data[i][1],payload={'raw_context':final_data[i][0] }) for i in range(len(final_data))]\n",
    "    qdrant_client.upsert(collection_name=\"CHATBOT\", points=points)\n",
    "    print(qdrant_client.get_collections())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def query_qdrant(query, collection_name='CHATBOT', limit=4):\n",
    "     \n",
    "    query_vector=embed([query]).data[0].embedding\n",
    "     \n",
    "    result = qdrant_client.search(\n",
    "        collection_name = collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit = limit,\n",
    "        with_vectors = False\n",
    "    )\n",
    "    # search_result=[]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_llm_context(result):\n",
    "    # result[0].payload['raw_context']\n",
    "    context =[]\n",
    "    for i in range(len(result)):\n",
    "        context.append(result[i].payload['raw_context'])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(context: list, query: str):\n",
    "    model = \"mistral-large-latest\"\n",
    "    \n",
    "    chat_response = client.chat.complete(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\n",
    "                \n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an answer generation agent, you'll be given context and query, generate answer in human readable form\",\n",
    "                \n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"here's the question {query} and here's the context {'--'.join(context)}\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    choice = input(\"Enter 'query' for rag query \\n 'entry' for rag entry\")\n",
    "    if choice == 'entry':\n",
    "        url: str = \"https://assets.techrepublic.com/uploads/2017/04/aexcelpowerbi.png\"\n",
    "        \n",
    "        context = generate_context(url)\n",
    "\n",
    "        final_data = generate_embeddings(context) \n",
    "        # final_data[1][1]--->size\n",
    "        initialize_qdrant(len(final_data[0][1]))\n",
    "\n",
    "        qdrant_entry(final_data)\n",
    "    else:\n",
    "        query = input(\"Enter the query : \")\n",
    "        result = query_qdrant(query)\n",
    "\n",
    "        context = prepare_llm_context(result)\n",
    "\n",
    "        response = query_llm(context, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
